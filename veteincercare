#1036 0215
"""
Ultra-Mega Reconciliation: Parameter-based with advanced Dashboard (8 charts).
This version includes:
  - Dashboard filters that correctly handle (NaN) for Start Date / End Date.
  - A dashboard that shows 8 charts as before.
  - PDF export: Each PDF page is letter size (8.5Ã—11 in) with a 1-inch margin,
    so the chart is plotted in one single axes covering the page (without a nested chart).
  - Run history is stored in the JSON config for the Band Chart.
"""

import os
import sys
import json
import math
import logging
import zipfile
import shutil
import io
from pathlib import Path
from datetime import datetime, timedelta
from typing import Dict, Set, List

import tkinter as tk
from tkinter import ttk, filedialog, messagebox
import customtkinter as ctk

import pandas as pd
import numpy as np

import matplotlib
matplotlib.use("TkAgg")
import matplotlib.pyplot as plt
from matplotlib.backends.backend_tkagg import FigureCanvasTkAgg
from matplotlib.backends.backend_pdf import PdfPages

try:
    import chardet
except ImportError:
    chardet = None

try:
    import psutil
except ImportError:
    psutil = None

from openpyxl import Workbook
from openpyxl.styles import PatternFill, Font, Alignment

from PIL import Image

# ----------------------------------------------------------------------------
# LOGGING
# ----------------------------------------------------------------------------
logging.basicConfig(level=logging.INFO, format="%(asctime)s [%(levelname)s] %(message)s")

# ----------------------------------------------------------------------------
# DEFAULT CONFIG & SAVE/LOAD (including run_history)
# ----------------------------------------------------------------------------
DEFAULT_PATHS = {
    "ERP_EXCEL_PATH": "data/ERP_Config.xlsx",
    "MASTER_ZIP_PATH": "data/Master_Config.zip",
    "EXCEPTION_PATH": "data/Exception_Table.xlsx",
    "OUTPUT_PATH": "output/missing_items.xlsx",
    "CONFIG_PATH": "config/ui_config.json",
    "PARAMETER_PATH": "data/parameters.xlsx",
    "MASTER_CSV_OUTPUT": "temp_master_csv",
    # For PDF export, we store the directory (our script will timestamp the filename)
    "PDF_EXPORT_PATH": "output/dashboard_reports",
    "LOGO_PATH": "images/company_logo.png"
}

def default_config() -> Dict:
    return {
        "paths": DEFAULT_PATHS.copy(),
        "erp_grid": {"filters": {}},
        "master_grid": {"filters": {}},
        # run_history will be a list of dicts with run timestamp and count
        "run_history": []
    }

def load_config(path: Path) -> Dict:
    if path.is_file():
        try:
            with open(path, "r", encoding="utf-8") as f:
                return json.load(f)
        except Exception as e:
            logging.warning(f"Could not load config: {e}")
    return default_config()

def save_config(cfg: Dict, path: Path):
    path.parent.mkdir(parents=True, exist_ok=True)
    try:
        # Convert sets -> lists in erp/master filters
        if "erp_grid" in cfg and "filters" in cfg["erp_grid"]:
            nf = {}
            for col, svals in cfg["erp_grid"]["filters"].items():
                nf[col] = list(svals)
            cfg["erp_grid"]["filters"] = nf
        if "master_grid" in cfg and "filters" in cfg["master_grid"]:
            nf = {}
            for col, svals in cfg["master_grid"]["filters"].items():
                nf[col] = list(svals)
            cfg["master_grid"]["filters"] = nf

        with open(path, "w", encoding="utf-8") as f:
            json.dump(cfg, f, indent=2)
        logging.info(f"Saved config to {path}")
    except Exception as e:
        logging.error(f"Error saving config: {e}")

# ----------------------------------------------------------------------------
# TEXT LOGGER HANDLER
# ----------------------------------------------------------------------------
class TextHandler(logging.Handler):
    def __init__(self, widget: ctk.CTkTextbox):
        super().__init__()
        self.widget = widget
    def emit(self, record):
        msg = self.format(record) + "\n"
        self.widget.after(0, self._append, msg)
    def _append(self, msg):
        self.widget.configure(state="normal")
        self.widget.insert("end", msg)
        self.widget.see("end")
        self.widget.configure(state="disabled")

# ----------------------------------------------------------------------------
# PARAMETER READING
# ----------------------------------------------------------------------------
def read_param_file(path: Path) -> Dict[str, object]:
    param = {
        "dim_erp_keep": set(),
        "dim_erp_map": {},
        "dim_master_map": {},
        "attr_erp_map": {},
        "attr_master_map": {}
    }
    if not path.is_file():
        logging.warning(f"Param file not found => {path}")
        return param
    try:
        dim_df = pd.read_excel(path, sheet_name="Dimension Parameters")
        dim_df.columns = dim_df.columns.astype(str).str.strip()
        def s(x): return str(x).strip() if pd.notna(x) else ""
        for _, row in dim_df.iterrows():
            fn = s(row.get("FileName", ""))
            vsc = s(row.get("V S C", ""))
            dim = s(row.get("Dimension", ""))
            ev  = s(row.get("ERP Values", ""))
            if ev.lower() == "x" and vsc and dim:
                param["dim_erp_keep"].add(vsc)
            if vsc and dim:
                param["dim_erp_map"][vsc] = dim
            if fn and dim and ev.lower() == "x":
                param["dim_master_map"][fn] = dim
        attr_df = pd.read_excel(path, sheet_name="Attribute Parameters")
        attr_df.columns = attr_df.columns.astype(str).str.strip()
        for _, row in attr_df.iterrows():
            e_orig = s(row.get("ERP Original Attributes", ""))
            m_orig = s(row.get("Master Original Attributes", ""))
            final_ = s(row.get("Attribute", ""))
            onoff  = s(row.get("On/Off", ""))
            if onoff.lower() == "x" and final_:
                if e_orig:
                    param["attr_erp_map"][e_orig] = final_
                if m_orig:
                    param["attr_master_map"][m_orig] = final_
        return param
    except Exception as e:
        logging.error(f"Error reading param file => {e}")
        return param

# ----------------------------------------------------------------------------
# ERP & MASTER READING
# ----------------------------------------------------------------------------
def read_erp_excel(path: Path) -> pd.DataFrame:
    if not path.is_file():
        logging.warning(f"ERP Excel not found => {path}")
        return pd.DataFrame()
    try:
        df = pd.read_excel(path, skiprows=3)
        df.columns = df.columns.str.strip()
        if "Enabled_Flag" in df.columns:
            df = df[df["Enabled_Flag"]=="Enabled"]
        return df
    except Exception as e:
        logging.error(f"Error reading ERP => {e}")
        return pd.DataFrame()

def read_txt_2encodings(raw: bytes) -> pd.DataFrame:
    import io
    for enc in ["utf-8-sig", "utf-16-le"]:
        try:
            buf = io.BytesIO(raw)
            df = pd.read_csv(buf, encoding=enc, on_bad_lines="skip", engine="python")
            df.dropna(how="all", axis=0, inplace=True)
            df.dropna(how="all", axis=1, inplace=True)
            df.columns = df.columns.astype(str).str.strip()
            logging.info(f"[read_txt_2encodings] success with {enc}, shape={df.shape}")
            return df
        except Exception as e:
            logging.debug(f"[read_txt_2encodings] fail with {enc} => {e}")
    logging.error("[read_txt_2encodings] cannot parse .txt => empty.")
    return pd.DataFrame()

def convert_master_txt_to_csv(zip_path: Path, out_dir: Path) -> List[Path]:
    if not zip_path.is_file():
        logging.warning(f"[Master] ZIP not found => {zip_path}")
        return []
    if out_dir.exists():
        shutil.rmtree(out_dir, ignore_errors=True)
    out_dir.mkdir(parents=True, exist_ok=True)
    csvs = []
    with zipfile.ZipFile(zip_path, "r") as z:
        txt_files = [f for f in z.namelist() if f.lower().endswith(".txt")]
        for txt_file in txt_files:
            base_name = os.path.basename(txt_file)
            if not base_name:
                continue
            try:
                with z.open(txt_file) as fo:
                    raw = fo.read()
                df = read_txt_2encodings(raw)
                if df.empty:
                    continue
                df["RawFileName"] = base_name
                if "Name" not in df.columns and len(df.columns) > 0:
                    first_col = df.columns[0]
                    df.rename(columns={first_col:"Name"}, inplace=True)
                out_csv = out_dir / (base_name.replace(".txt", ".csv"))
                df.to_csv(out_csv, index=False, encoding="utf-8")
                csvs.append(out_csv)
            except Exception as e:
                logging.error(f"[Master] error reading {txt_file} => {e}")
    return csvs

def unify_master_csvs(csvs: List[Path]) -> pd.DataFrame:
    frames = []
    for cp in csvs:
        if not cp.is_file():
            continue
        try:
            df = pd.read_csv(cp, encoding="utf-8", on_bad_lines="skip")
            df.columns = df.columns.str.strip()
            frames.append(df)
        except Exception as e:
            logging.error(f"[unify_master_csvs] reading {cp} => {e}")
    if frames:
        return pd.concat(frames, ignore_index=True)
    return pd.DataFrame()

# ----------------------------------------------------------------------------
# MELTDOWN FUNCTIONS (to convert wide to long)
# ----------------------------------------------------------------------------
def meltdown_erp_for_preview(df: pd.DataFrame, param: Dict[str, object]) -> pd.DataFrame:
    if "V_S_C" not in df.columns:
        return pd.DataFrame()
    keep = param.get("dim_erp_keep", set())
    dmap = param.get("dim_erp_map", {})
    amap = param.get("attr_erp_map", {})

    df2 = df[df["V_S_C"].isin(keep)].copy()
    if df2.empty:
        return pd.DataFrame()

    skip_cols = {"V_S_C", "Enabled_Flag"}
    id_vars = []
    if "Value" in df2.columns:
        id_vars.append("Value")
        skip_cols.add("Value")
    df2["DimRaw"] = df2["V_S_C"]
    skip_cols.add("DimRaw")
    id_vars.insert(0, "DimRaw")

    meltdown_cols = [c for c in df2.columns if c not in skip_cols]
    melted = df2.melt(id_vars=id_vars, value_vars=meltdown_cols,
                      var_name="OrigAttr", value_name="ValX")
    def rename_dim(v):
        return dmap.get(v, v)
    melted["Dimension"] = melted["DimRaw"].apply(rename_dim)
    if "Value" in id_vars:
        melted.rename(columns={"Value": "Name"}, inplace=True)
    else:
        melted["Name"] = ""
    melted = melted[melted["OrigAttr"].isin(amap.keys())].copy()
    melted["Attribute"] = melted["OrigAttr"].map(amap)
    def strip_t(val):
        if isinstance(val, str) and "T" in val:
            return val.split("T")[0]
        return val
    melted["Value"] = np.where(melted["Attribute"].isin(["Start Date", "End Date"]),
                               melted["ValX"].apply(strip_t), melted["ValX"])
    return melted[["Dimension", "Name", "Attribute", "Value"]]

def meltdown_master_for_preview(df: pd.DataFrame, param: Dict[str, object]) -> pd.DataFrame:
    if df.empty or "RawFileName" not in df.columns:
        return pd.DataFrame()
    keep_map = param.get("dim_master_map", {})
    amap = param.get("attr_master_map", {})

    df2 = df[df["RawFileName"].isin(keep_map.keys())].copy()
    if df2.empty:
        return pd.DataFrame()

    df2["DimRaw"] = df2["RawFileName"]

    skip_cols = {"RawFileName", "DimRaw"}
    id_vars = ["DimRaw"]
    if "Name" in df2.columns:
        id_vars.append("Name")
        skip_cols.add("Name")

    meltdown_cols = [c for c in df2.columns if c not in skip_cols]
    melted = df2.melt(id_vars=id_vars, value_vars=meltdown_cols,
                      var_name="OrigAttr", value_name="ValX")
    def rename_dim(fn):
        return keep_map.get(fn, fn)
    melted["Dimension"] = melted["DimRaw"].apply(rename_dim)
    if "Name" in id_vars:
        melted.rename(columns={"Name": "Name"}, inplace=True)
    else:
        melted["Name"] = ""
    melted = melted[melted["OrigAttr"].isin(amap.keys())].copy()
    melted["Attribute"] = melted["OrigAttr"].map(amap)
    def strip_t(val):
        if isinstance(val, str) and "T" in val:
            return val.split("T")[0]
        return val
    melted["Value"] = np.where(melted["Attribute"].isin(["Start Date", "End Date"]),
                               melted["ValX"].apply(strip_t), melted["ValX"])
    return melted[["Dimension", "Name", "Attribute", "Value"]]

def pivot_for_preview(df: pd.DataFrame) -> pd.DataFrame:
    if not df.empty and {"Dimension", "Name", "Attribute"}.issubset(df.columns):
        df = df.drop_duplicates(subset=["Dimension", "Name", "Attribute"])
        try:
            df = df.pivot(index=["Dimension", "Name"], columns="Attribute", values="Value").reset_index()
        except Exception as e:
            logging.error(f"Pivot error => {e}")
    return df

# ----------------------------------------------------------------------------
# Compare Functions: Melt back wide data and produce missing items
# ----------------------------------------------------------------------------
def melt_back(df: pd.DataFrame) -> pd.DataFrame:
    if df.empty or "Dimension" not in df.columns or "Name" not in df.columns:
        return pd.DataFrame()
    skip_cols = {"Dimension", "Name"}
    meltdown_cols = [c for c in df.columns if c not in skip_cols]
    melted = df.melt(id_vars=["Dimension", "Name"], value_vars=meltdown_cols,
                     var_name="Attribute", value_name="Value")
    return melted[["Dimension", "Name", "Attribute", "Value"]]

def build_keys(df: pd.DataFrame) -> pd.DataFrame:
    df = df.copy()
    for c in ["Dimension", "Name", "Attribute", "Value"]:
        if c not in df.columns:
            df[c] = ""
        df[c] = df[c].fillna("").astype(str).str.strip()
    df["GroupKey"] = df["Dimension"] + " | " + df["Name"]
    df["Key"] = df["Dimension"] + " | " + df["Name"] + " | " + df["Attribute"] + " | " + df["Value"]
    df["Comments_1"] = ""
    df["Comments_2"] = ""
    df["Action Item"] = ""
    df["Missing In"] = ""
    return df

def compare_mode2(df_erp: pd.DataFrame, df_mst: pd.DataFrame) -> pd.DataFrame:
    def to_dict(d):
        out = {}
        for gk, grp in d.groupby("GroupKey"):
            rec = {}
            nm = grp["Name"].iloc[0] if not grp.empty else ""
            rec["Name"] = nm
            for _, row in grp.iterrows():
                rec[row["Attribute"]] = row["Value"]
            out[gk] = rec
        return out
    e_dict = to_dict(df_erp)
    m_dict = to_dict(df_mst)
    all_gk = set(e_dict.keys()) | set(m_dict.keys())
    results = []
    for gk in all_gk:
        dim = gk.split(" | ")[0]
        a_data = e_dict.get(gk, {})
        b_data = m_dict.get(gk, {})
        name_a = a_data.get("Name", "")
        name_b = b_data.get("Name", "")
        if name_a and name_b and name_a == name_b:
            all_attrs = (set(a_data.keys()) | set(b_data.keys())) - {"Name"}
            for at in all_attrs:
                va = a_data.get(at, "")
                vb = b_data.get(at, "")
                if va != vb:
                    if va and not vb:
                        results.append({"Dimension": dim, "Name": name_a, "Attribute": at, "Value": va, "Missing In": "MASTER"})
                    elif vb and not va:
                        results.append({"Dimension": dim, "Name": name_a, "Attribute": at, "Value": vb, "Missing In": "ERP"})
                    else:
                        results.append({"Dimension": dim, "Name": name_a, "Attribute": at, "Value": va, "Missing In": "MASTER"})
                        results.append({"Dimension": dim, "Name": name_a, "Attribute": at, "Value": vb, "Missing In": "ERP"})
        else:
            if name_a and not name_b:
                results.append({"Dimension": dim, "Name": name_a, "Attribute": "Name", "Value": name_a, "Missing In": "MASTER"})
            elif name_b and not name_a:
                results.append({"Dimension": dim, "Name": name_b, "Attribute": "Name", "Value": name_b, "Missing In": "ERP"})
    df_res = pd.DataFrame(results)
    if not df_res.empty:
        df_res["Key"] = (df_res["Dimension"].str.strip() + " | " +
                         df_res["Name"].str.strip() + " | " +
                         df_res["Attribute"].str.strip() + " | " +
                         df_res["Value"].str.strip())
    return df_res
